{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2bb0eaa",
   "metadata": {},
   "source": [
    "# Infini-gram Document Search: N-gram to Document Retrieval\n",
    "\n",
    "This notebook demonstrates how to extract n-grams from input text and retrieve the top 3 documents containing each n-gram using the Infini-gram API.\n",
    "\n",
    "## Workflow:\n",
    "1. Extract n-grams from input text (2-gram to 4-gram)\n",
    "2. For each n-gram, query the Infini-gram API to find matching documents\n",
    "3. Retrieve the top 3 documents for each n-gram\n",
    "4. Display results with document text and metadata\n",
    "\n",
    "## Requirements:\n",
    "- Access to Infini-gram API (https://api.infini-gram.io/)\n",
    "- Python packages: requests, transformers (for tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b66fc1",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import required packages and configure API settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17e3682e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Setup complete\n",
      "ğŸ“¡ API Base URL: https://api.infini-gram.io/\n",
      "ğŸ“Š Using index: v4_rpj_llama_s4\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from collections import defaultdict\n",
    "\n",
    "# API configuration\n",
    "API_BASE_URL = \"https://api.infini-gram.io/\"\n",
    "SELECTED_INDEX = \"v4_rpj_llama_s4\"  # RedPajama with Llama-2 tokenizer\n",
    "\n",
    "print(\"âœ… Setup complete\")\n",
    "print(f\"ğŸ“¡ API Base URL: {API_BASE_URL}\")\n",
    "print(f\"ğŸ“Š Using index: {SELECTED_INDEX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4c488e",
   "metadata": {},
   "source": [
    "## 2. API Access Verification\n",
    "\n",
    "Test API connectivity and verify the selected index is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c75cd764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API is working! Test count: 332,456,956\n",
      "ğŸš€ Ready to search documents!\n"
     ]
    }
   ],
   "source": [
    "def check_api_access():\n",
    "    \"\"\"Verify API access with a simple test query\"\"\"\n",
    "    try:\n",
    "        payload = {\n",
    "            'index': SELECTED_INDEX,\n",
    "            'query_type': 'count',\n",
    "            'query': 'the'\n",
    "        }\n",
    "        \n",
    "        response = requests.post(API_BASE_URL, json=payload, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            if 'error' not in result:\n",
    "                print(f\"âœ… API is working! Test count: {result.get('count', 'N/A'):,}\")\n",
    "                return True\n",
    "        \n",
    "        print(f\"âŒ API check failed: {response.status_code}\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ API error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test API access\n",
    "api_working = check_api_access()\n",
    "if api_working:\n",
    "    print(\"ğŸš€ Ready to search documents!\")\n",
    "else:\n",
    "    print(\"âš ï¸ API issues detected - some functions may not work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20898d81",
   "metadata": {},
   "source": [
    "## 3. N-gram Extraction\n",
    "\n",
    "Extract n-grams from input text for document search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9aeb25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple tokenization for demonstration purposes\"\"\"\n",
    "    # Clean and tokenize the text\n",
    "    return text.lower().split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0c25c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_ngrams(text: str, min_n: int = 1, max_n: int = 5) -> Dict[int, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract n-grams from text using simple word-based tokenization\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to extract n-grams from\n",
    "        min_n: Minimum n-gram size\n",
    "        max_n: Maximum n-gram size\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping n-gram size to list of n-grams\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = simple_tokenize(text)\n",
    "    \n",
    "    ngrams = {}\n",
    "    \n",
    "    for n in range(min_n, max_n + 1):\n",
    "        ngram_list = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = ' '.join(tokens[i:i + n])\n",
    "            ngram_list.append(ngram)\n",
    "        ngrams[n] = ngram_list\n",
    "    \n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0626449c",
   "metadata": {},
   "source": [
    "## 4. Document Retrieval\n",
    "\n",
    "Core functionality to retrieve top 3 documents for each n-gram from input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "92f52caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_documents_for_ngrams(text: str, max_ngram_size: int = 4, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Extract n-grams from text and get top K documents for each n-gram\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to extract n-grams from\n",
    "        max_ngram_size: Maximum n-gram size (default: 4)\n",
    "        top_k: Number of top documents to retrieve per n-gram (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        List of results, one per n-gram with top documents\n",
    "    \"\"\"\n",
    "    # Extract n-grams\n",
    "    ngrams = extract_ngrams(text, min_n=2, max_n=max_ngram_size)\n",
    "    \n",
    "    # Flatten n-grams and prioritize longer ones\n",
    "    all_ngrams = []\n",
    "    for n in sorted(ngrams.keys(), reverse=True):\n",
    "        for ngram in ngrams[n][:5]:  # Limit to 5 per size\n",
    "            if ngram not in all_ngrams:\n",
    "                all_ngrams.append(ngram)\n",
    "    \n",
    "    print(f\"ğŸ” Processing {len(all_ngrams)} unique n-grams\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, ngram in enumerate(all_ngrams, 1):\n",
    "        print(f\"\\nğŸ“¡ {i}/{len(all_ngrams)}: Searching '{ngram}'\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Use 'find' query to get documents containing this n-gram\n",
    "            find_payload = {\n",
    "                'index': SELECTED_INDEX,\n",
    "                'query_type': 'find',\n",
    "                'query': ngram\n",
    "            }\n",
    "            \n",
    "            response = requests.post(API_BASE_URL, json=find_payload, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"   âŒ Find query failed: {response.status_code}\")\n",
    "                continue\n",
    "            \n",
    "            result = response.json()\n",
    "            if 'error' in result:\n",
    "                print(f\"   âŒ API Error: {result['error']}\")\n",
    "                continue\n",
    "            \n",
    "            count = result.get('cnt', 0)\n",
    "            segments = result.get('segment_by_shard', [])\n",
    "            \n",
    "            print(f\"   ğŸ“Š Found {count:,} occurrences across {len(segments)} shards\")\n",
    "            \n",
    "            if count == 0:\n",
    "                print(f\"   ğŸ“­ No documents found\")\n",
    "                results.append({\n",
    "                    'ngram': ngram,\n",
    "                    'ngram_length': len(ngram.split()),\n",
    "                    'total_count': count,\n",
    "                    'documents_retrieved': 0,\n",
    "                    'documents': []\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Step 2: Retrieve actual documents from the segments\n",
    "            documents = []\n",
    "            retrieved = 0\n",
    "            \n",
    "            for shard_idx, (start_rank, end_rank) in enumerate(segments):\n",
    "                if retrieved >= top_k:\n",
    "                    break\n",
    "                \n",
    "                # Try several ranks from this shard\n",
    "                ranks_to_try = list(range(start_rank, min(start_rank + 5, end_rank + 1)))\n",
    "                \n",
    "                for rank in ranks_to_try:\n",
    "                    if retrieved >= top_k:\n",
    "                        break\n",
    "                    \n",
    "                    try:\n",
    "                        # Try to get document by rank using the method we discovered works\n",
    "                        doc_payload = {\n",
    "                            'index': SELECTED_INDEX,\n",
    "                            'query_type': 'get_doc_by_rank',\n",
    "                            'query': ngram,  # Include the query as required\n",
    "                            's': shard_idx,\n",
    "                            'rank': rank\n",
    "                        }\n",
    "                        \n",
    "                        doc_response = requests.post(API_BASE_URL, json=doc_payload, timeout=10)\n",
    "                        \n",
    "                        if doc_response.status_code == 200:\n",
    "                            doc_result = doc_response.json()\n",
    "                            \n",
    "                            if 'error' not in doc_result:\n",
    "                                # Extract document text from spans\n",
    "                                doc_text = \"\"\n",
    "                                if 'spans' in doc_result:\n",
    "                                    spans = doc_result['spans']\n",
    "                                    doc_text = \" \".join([span[0] for span in spans if span and len(span) > 0 and span[0]])\n",
    "                                \n",
    "                                if doc_text.strip():  # Only add if we got actual text\n",
    "                                    documents.append({\n",
    "                                        'rank': rank,\n",
    "                                        'shard': shard_idx,\n",
    "                                        'text': doc_text,\n",
    "                                        'text_preview': doc_text[:300] + \"...\" if len(doc_text) > 300 else doc_text,\n",
    "                                        'doc_length': len(doc_text.split()),\n",
    "                                        'doc_ix': doc_result.get('doc_ix', 'unknown')\n",
    "                                    })\n",
    "                                    retrieved += 1\n",
    "                                    print(f\"   âœ… Retrieved document {retrieved}/{top_k} (rank: {rank})\")\n",
    "                                    \n",
    "                                    # Short break between requests to avoid overloading\n",
    "                                    import time\n",
    "                                    time.sleep(0.1)\n",
    "                            else:\n",
    "                                print(f\"   âš ï¸ Error in document result: {doc_result.get('error', 'unknown')}\")\n",
    "                        else:\n",
    "                            print(f\"   âš ï¸ Document request failed: {doc_response.status_code}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"   âš ï¸ Error getting document at rank {rank}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Add result for this n-gram\n",
    "            ngram_result = {\n",
    "                'ngram': ngram,\n",
    "                'ngram_length': len(ngram.split()),\n",
    "                'total_count': count,\n",
    "                'documents_retrieved': len(documents),\n",
    "                'documents': documents\n",
    "            }\n",
    "            \n",
    "            results.append(ngram_result)\n",
    "            \n",
    "            if len(documents) > 0:\n",
    "                print(f\"   âœ… Successfully retrieved {len(documents)} documents with text\")\n",
    "            else:\n",
    "                print(f\"   âš ï¸ Found {count:,} occurrences but couldn't retrieve document text\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Failed to process '{ngram}': {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Search completed: {len(results)} n-grams processed\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaf1c96",
   "metadata": {},
   "source": [
    "### Note on Document Retrieval\n",
    "\n",
    "The current implementation verifies that n-grams exist in the corpus and returns the counts. \n",
    "To retrieve actual document text, you would need to:\n",
    "\n",
    "1. Use the `find` query to get document segments\n",
    "2. Use appropriate API endpoints to retrieve document content by rank/shard\n",
    "3. Extract text from the returned spans\n",
    "\n",
    "The framework above shows the structure - you can extend the document retrieval part based on your specific API access and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d85ef7",
   "metadata": {},
   "source": [
    "### Understanding \"Rank\" in Infini-gram\n",
    "\n",
    "In the Infini-gram API, **\"rank\"** refers to the position/index of a document within the corpus. Here's how it works:\n",
    "\n",
    "1. **Global Document Ordering**: Every document in the corpus has a unique rank (position number)\n",
    "2. **Shard-based Organization**: Documents are distributed across multiple shards for efficiency\n",
    "3. **Segment Ranges**: When you search for an n-gram, the API returns `segment_by_shard` which contains:\n",
    "   - `[start_rank, end_rank]` pairs for each shard\n",
    "   - These define ranges of document ranks that contain your n-gram\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "segments = [[232539299067, 232539338892], [232516639042, 232516678886]]\n",
    "#            ^start_rank    ^end_rank      ^start_rank    ^end_rank\n",
    "#            Documents 232539299067 to    Documents 232516639042 to  \n",
    "#            232539338892 contain the      232516678886 contain the\n",
    "#            n-gram in shard 0             n-gram in shard 1\n",
    "```\n",
    "\n",
    "**To retrieve a specific document**: You use `get_doc_by_rank` with:\n",
    "- `s`: shard index (0, 1, 2, etc.)\n",
    "- `rank`: specific document rank within that range\n",
    "- `query`: the original n-gram (required by API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21dd8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(results: List[Dict]):\n",
    "    \"\"\"Display search results in a clean, readable format\"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        print(\"ğŸ“­ No results found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nğŸ“Š DOCUMENT SEARCH RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nğŸ” N-gram {i}: '{result['ngram']}'\")\n",
    "        print(f\"   ğŸ“ˆ Total occurrences: {result['total_count']:,}\")\n",
    "        print(f\"   ğŸ“„ Documents retrieved: {result['documents_retrieved']}\")\n",
    "        \n",
    "        if result['documents']:\n",
    "            print(f\"   ğŸ“ Top documents:\")\n",
    "            \n",
    "            for j, doc in enumerate(result['documents'], 1):\n",
    "                print(f\"\\n      ğŸ“„ Document {j}:\")\n",
    "                print(f\"         ğŸ¯ Rank: {doc['rank']}, Shard: {doc['shard']}\")\n",
    "                print(f\"         ğŸ“ Length: {doc['doc_length']} words\")\n",
    "                print(f\"         ğŸ“ƒ Text: {doc['text_preview']}\")\n",
    "        else:\n",
    "            print(f\"   âŒ No documents retrieved\")\n",
    "        \n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa6bf0",
   "metadata": {},
   "source": [
    "## 5. Example Usage\n",
    "\n",
    "Test the document retrieval functionality with sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "670583d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting document retrieval...\n",
      "ğŸ“ Input text: Artificial intelligence and machine learning are transforming how we process natural language.\n",
      "Neural networks can now understand context and generate human-like responses.\n",
      "ğŸ” Processing 10 unique n-grams\n",
      "\n",
      "ğŸ“¡ 1/10: Searching 'artificial intelligence and'\n",
      "   ğŸ“Š Found 3,640 occurrences across 4 shards\n",
      "   ğŸ“Š Found 3,640 occurrences across 4 shards\n",
      "   âœ… Retrieved document 1/3 (rank: 232539306669)\n",
      "   âœ… Retrieved document 1/3 (rank: 232539306669)\n",
      "   âœ… Retrieved document 2/3 (rank: 232539306670)\n",
      "   âœ… Retrieved document 2/3 (rank: 232539306670)\n",
      "   âœ… Retrieved document 3/3 (rank: 232539306671)\n",
      "   âœ… Successfully retrieved 3 documents with text\n",
      "\n",
      "ğŸ“¡ 2/10: Searching 'intelligence and machine'\n",
      "   âœ… Retrieved document 3/3 (rank: 232539306671)\n",
      "   âœ… Successfully retrieved 3 documents with text\n",
      "\n",
      "ğŸ“¡ 2/10: Searching 'intelligence and machine'\n",
      "   ğŸ“Š Found 360 occurrences across 4 shards\n",
      "   ğŸ“Š Found 360 occurrences across 4 shards\n",
      "   âœ… Retrieved document 1/3 (rank: 19605443884)\n",
      "   âœ… Retrieved document 1/3 (rank: 19605443884)\n",
      "   âœ… Retrieved document 2/3 (rank: 19605443885)\n",
      "   âœ… Retrieved document 2/3 (rank: 19605443885)\n",
      "   âœ… Retrieved document 3/3 (rank: 19605443886)\n",
      "   âœ… Successfully retrieved 3 documents with text\n",
      "\n",
      "ğŸ“¡ 3/10: Searching 'and machine learning'\n",
      "   âœ… Retrieved document 3/3 (rank: 19605443886)\n",
      "   âœ… Successfully retrieved 3 documents with text\n",
      "\n",
      "ğŸ“¡ 3/10: Searching 'and machine learning'\n",
      "   ğŸ“Š Found 1,615 occurrences across 4 shards\n",
      "   ğŸ“Š Found 1,615 occurrences across 4 shards\n",
      "   âœ… Retrieved document 1/3 (rank: 179595127225)\n",
      "   âœ… Retrieved document 1/3 (rank: 179595127225)\n",
      "   âœ… Retrieved document 2/3 (rank: 179595127226)\n",
      "   âœ… Retrieved document 2/3 (rank: 179595127226)\n",
      "   âœ… Retrieved document 3/3 (rank: 179595127227)\n",
      "   âœ… Successfully retrieved 3 documents with text\n",
      "\n",
      "ğŸ“¡ 4/10: Searching 'machine learning are'\n",
      "   âœ… Retrieved document 3/3 (rank: 179595127227)\n",
      "   âœ… Successfully retrieved 3 documents with text\n",
      "\n",
      "ğŸ“¡ 4/10: Searching 'machine learning are'\n",
      "   ğŸ“Š Found 136 occurrences across 4 shards\n",
      "   ğŸ“Š Found 136 occurrences across 4 shards\n",
      "   âœ… Retrieved document 1/3 (rank: 313625094008)\n",
      "   âœ… Retrieved document 1/3 (rank: 313625094008)\n",
      "   âœ… Retrieved document 2/3 (rank: 313625094009)\n",
      "   âœ… Retrieved document 2/3 (rank: 313625094009)\n",
      "   âœ… Retrieved document 3/3 (rank: 313625094010)\n",
      "   âœ… Successfully retrieved 3 documents with text\n",
      "\n",
      "ğŸ“¡ 5/10: Searching 'learning are transforming'\n",
      "   âœ… Retrieved document 3/3 (rank: 313625094010)\n",
      "   âœ… Successfully retrieved 3 documents with text\n",
      "\n",
      "ğŸ“¡ 5/10: Searching 'learning are transforming'\n",
      "   ğŸ“Š Found 13 occurrences across 4 shards\n",
      "   ğŸ“Š Found 13 occurrences across 4 shards\n",
      "   âœ… Retrieved document 1/3 (rank: 175652238324)\n",
      "   âœ… Retrieved document 1/3 (rank: 175652238324)\n",
      "   âœ… Retrieved document 2/3 (rank: 175652238325)\n",
      "   âœ… Retrieved document 2/3 (rank: 175652238325)\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âœ… Successfully retrieved 2 documents with text\n",
      "\n",
      "ğŸ“¡ 6/10: Searching 'artificial intelligence'\n",
      "   âš ï¸ Document request failed: 429\n",
      "   âœ… Successfully retrieved 2 documents with text\n",
      "\n",
      "ğŸ“¡ 6/10: Searching 'artificial intelligence'\n",
      "   âŒ Find query failed: 429\n",
      "\n",
      "ğŸ“¡ 7/10: Searching 'intelligence and'\n",
      "   âŒ Find query failed: 429\n",
      "\n",
      "ğŸ“¡ 7/10: Searching 'intelligence and'\n",
      "   âŒ Find query failed: 429\n",
      "\n",
      "ğŸ“¡ 8/10: Searching 'and machine'\n",
      "   âŒ Find query failed: 429\n",
      "\n",
      "ğŸ“¡ 8/10: Searching 'and machine'\n",
      "   âŒ Find query failed: 429\n",
      "\n",
      "ğŸ“¡ 9/10: Searching 'machine learning'\n",
      "   âŒ Find query failed: 429\n",
      "\n",
      "ğŸ“¡ 9/10: Searching 'machine learning'\n",
      "   âŒ Find query failed: 429\n",
      "\n",
      "ğŸ“¡ 10/10: Searching 'learning are'\n",
      "   âŒ Find query failed: 429\n",
      "\n",
      "ğŸ“¡ 10/10: Searching 'learning are'\n",
      "   âŒ Find query failed: 429\n",
      "\n",
      "ğŸ“‹ Search completed: 5 n-grams processed\n",
      "\n",
      "ğŸ“Š DOCUMENT SEARCH RESULTS\n",
      "================================================================================\n",
      "\n",
      "ğŸ” N-gram 1: 'artificial intelligence and'\n",
      "   ğŸ“ˆ Total occurrences: 3,640\n",
      "   ğŸ“„ Documents retrieved: 3\n",
      "   ğŸ“ Top documents:\n",
      "\n",
      "      ğŸ“„ Document 1:\n",
      "         ğŸ¯ Rank: 232539306669, Shard: 0\n",
      "         ğŸ“ Length: 497 words\n",
      "         ğŸ“ƒ Text: (small search engines do this)\n",
      "Indexing them, but only storing relevant pages in barrels (this is done by Google and Seznam, for example)\n",
      "Index them as regular words and create complete barrels (this solution has the problem that barrels can easily exceed the size of a regular hard drive and therefo...\n",
      "\n",
      "      ğŸ“„ Document 2:\n",
      "         ğŸ¯ Rank: 232539306670, Shard: 0\n",
      "         ğŸ“ Length: 708 words\n",
      "         ğŸ“ƒ Text: its, Turkey is taking a step and testing the online visit system in a few pilot institutions. The Minister of Justice has announced that once use of the system becomes prevalent, inmates will be able to hold visual call with their families for up to 30 minutes a week. For the time being, online visu...\n",
      "\n",
      "      ğŸ“„ Document 3:\n",
      "         ğŸ¯ Rank: 232539306671, Shard: 0\n",
      "         ğŸ“ Length: 269 words\n",
      "         ğŸ“ƒ Text: \\section{Introduction}\n",
      "\n",
      "Eliciting predictions of uncertain events from \\emph{experts}\n",
      "or other knowledgeable agents---or \n",
      "relevant information pertaining to events---is a fundamental \n",
      "problem of study in statistics, economics, operations research,\n",
      " artificial intelligence and a variety of other area...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ” N-gram 2: 'intelligence and machine'\n",
      "   ğŸ“ˆ Total occurrences: 360\n",
      "   ğŸ“„ Documents retrieved: 3\n",
      "   ğŸ“ Top documents:\n",
      "\n",
      "      ğŸ“„ Document 1:\n",
      "         ğŸ¯ Rank: 19605443884, Shard: 0\n",
      "         ğŸ“ Length: 671 words\n",
      "         ğŸ“ƒ Text: Memorial Healthcare System\n",
      "Ganesh Persad is the Manager of Clinical Systems & Interoperability at Memorial Healthcare System (MHS). He serves as the technical architect who manages various health information exchange platforms such as Epicâ€™s Care Everywhere, Carequality, eHealth Exchange, Florida He...\n",
      "\n",
      "      ğŸ“„ Document 2:\n",
      "         ğŸ¯ Rank: 19605443885, Shard: 0\n",
      "         ğŸ“ Length: 703 words\n",
      "         ğŸ“ƒ Text: said.\n",
      "â€œWe are going to build the secure, intelligent platform off which you can run your digital business of the future,â€ Robbins said.\n",
      "The new kind of intuitive network is powered by intent and informed by context. What does that mean? Imagine you are an IT or network administrator and want to depl...\n",
      "\n",
      "      ğŸ“„ Document 3:\n",
      "         ğŸ¯ Rank: 19605443886, Shard: 0\n",
      "         ğŸ“ Length: 703 words\n",
      "         ğŸ“ƒ Text: said.\n",
      "â€œWe are going to build the secure, intelligent platform off which you can run your digital business of the future,â€ Robbins said.\n",
      "The new kind of intuitive network is powered by intent and informed by context. What does that mean? Imagine you are an IT or network administrator and want to depl...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ” N-gram 3: 'and machine learning'\n",
      "   ğŸ“ˆ Total occurrences: 1,615\n",
      "   ğŸ“„ Documents retrieved: 3\n",
      "   ğŸ“ Top documents:\n",
      "\n",
      "      ğŸ“„ Document 1:\n",
      "         ğŸ¯ Rank: 179595127225, Shard: 0\n",
      "         ğŸ“ Length: 695 words\n",
      "         ğŸ“ƒ Text: quire Human Element due to Domain Complexity and Poor Process Discipline\n",
      "By 2025, 30% of new legal technology automation solutions will combine software with staffing for a â€œhuman-in-the-loopâ€ offering, according to Gartner, Inc. Despite growing demand for greater automation and increasing sophistic...\n",
      "\n",
      "      ğŸ“„ Document 2:\n",
      "         ğŸ¯ Rank: 179595127226, Shard: 0\n",
      "         ğŸ“ Length: 621 words\n",
      "         ğŸ“ƒ Text: ï¿½ï¿½,ç„¡è«–æ˜¯ç·šä¸Šç”¢æ¥­ä¸å¯æˆ–ç¼ºçš„å¤§æ•¸æ“šæ•´åˆã€é æ¸¬è¡Œæ¥­è¶¨å‹¢æä¾›æœ‰æ•ˆè³‡è¨Š;æˆ–æ˜¯è³‡é‡‘\n",
      "èˆ‡æ•¸æ“šçš„å®‰å…¨ä¿è­·å•é¡Œ,äº¦èƒ½åˆ©ç”¨AIä¾†é˜²æ­¢ç¶²è·¯æ”»æ“Šã€‚ä¸”çœ‹ä»¥ä¸‹äº”å¤§å°‡å½±éŸ¿åšå¥•ç”¢æ¥­çš„AIæŠ€è¡“! !\n",
      "As the development of artificial intelligence grows increasingly more sophisticated and begins to become\n",
      "embedded in a number of solutions, the potential for its use in the gambling sector is becoming more\n",
      "apparent...\n",
      "\n",
      "      ğŸ“„ Document 3:\n",
      "         ğŸ¯ Rank: 179595127227, Shard: 0\n",
      "         ğŸ“ Length: 467 words\n",
      "         ğŸ“ƒ Text: Help doctors to make better medical decisions, how does machine learning work?\n",
      "Home > Technology > Machine learning > Help doctors to make better medical decisions, how does machine learning work?\n",
      "In most cases, big data is regarded by one of the major hospitals as one of the least important abiliti...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ” N-gram 4: 'machine learning are'\n",
      "   ğŸ“ˆ Total occurrences: 136\n",
      "   ğŸ“„ Documents retrieved: 3\n",
      "   ğŸ“ Top documents:\n",
      "\n",
      "      ğŸ“„ Document 1:\n",
      "         ğŸ¯ Rank: 313625094008, Shard: 0\n",
      "         ğŸ“ Length: 767 words\n",
      "         ğŸ“ƒ Text: technologies such as artificial intelligence (AI) and machine learning (ML).\n",
      "AI and ML are used to make intelligent decisions about key business and banking processes. They may also be used to analyze large amounts of data in order to determine the creditworthiness of an application. Additionally, A...\n",
      "\n",
      "      ğŸ“„ Document 2:\n",
      "         ğŸ¯ Rank: 313625094009, Shard: 0\n",
      "         ğŸ“ Length: 651 words\n",
      "         ğŸ“ƒ Text: emost goal of mobile service providers. In Korea, we have already achieved that.â€ With 5G the network â€œwill be a lot of different types of traffic that need to be able to connect. In order to support those different types of traffic â€¦ it will require a lot of work. Thatâ€™s why we are looking at data ...\n",
      "\n",
      "      ğŸ“„ Document 3:\n",
      "         ğŸ¯ Rank: 313625094010, Shard: 0\n",
      "         ğŸ“ Length: 376 words\n",
      "         ğŸ“ƒ Text: up the rate of advanced prostate cancers. Another problem with skipping screening is that it can deny men with low-grade prostate cancers the option of undergoing active surveillance so they will know if their disease starts growing quickly and needs more aggressive therapy,\" he said.\n",
      "In the years a...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ” N-gram 5: 'learning are transforming'\n",
      "   ğŸ“ˆ Total occurrences: 13\n",
      "   ğŸ“„ Documents retrieved: 2\n",
      "   ğŸ“ Top documents:\n",
      "\n",
      "      ğŸ“„ Document 1:\n",
      "         ğŸ¯ Rank: 175652238324, Shard: 0\n",
      "         ğŸ“ Length: 664 words\n",
      "         ğŸ“ƒ Text: ing keynote: Redefining power\n",
      "Nilanjana Roy, Novelist and FT Life & Arts columnist\n",
      "Money management: the great missed opportunity\n",
      "Women are vastly outnumbered by men in asset management. They occupy an estimated one in ten roles, and about 90 per cent of the money in funds is managed by men. Why doe...\n",
      "\n",
      "      ğŸ“„ Document 2:\n",
      "         ğŸ¯ Rank: 175652238325, Shard: 0\n",
      "         ğŸ“ Length: 664 words\n",
      "         ğŸ“ƒ Text: ing keynote: Redefining power\n",
      "Nilanjana Roy, Novelist and FT Life & Arts columnist\n",
      "Money management: the great missed opportunity\n",
      "Women are vastly outnumbered by men in asset management. They occupy an estimated one in ten roles, and about 90 per cent of the money in funds is managed by men. Why doe...\n",
      "------------------------------------------------------------\n",
      "   âŒ Find query failed: 429\n",
      "\n",
      "ğŸ“‹ Search completed: 5 n-grams processed\n",
      "\n",
      "ğŸ“Š DOCUMENT SEARCH RESULTS\n",
      "================================================================================\n",
      "\n",
      "ğŸ” N-gram 1: 'artificial intelligence and'\n",
      "   ğŸ“ˆ Total occurrences: 3,640\n",
      "   ğŸ“„ Documents retrieved: 3\n",
      "   ğŸ“ Top documents:\n",
      "\n",
      "      ğŸ“„ Document 1:\n",
      "         ğŸ¯ Rank: 232539306669, Shard: 0\n",
      "         ğŸ“ Length: 497 words\n",
      "         ğŸ“ƒ Text: (small search engines do this)\n",
      "Indexing them, but only storing relevant pages in barrels (this is done by Google and Seznam, for example)\n",
      "Index them as regular words and create complete barrels (this solution has the problem that barrels can easily exceed the size of a regular hard drive and therefo...\n",
      "\n",
      "      ğŸ“„ Document 2:\n",
      "         ğŸ¯ Rank: 232539306670, Shard: 0\n",
      "         ğŸ“ Length: 708 words\n",
      "         ğŸ“ƒ Text: its, Turkey is taking a step and testing the online visit system in a few pilot institutions. The Minister of Justice has announced that once use of the system becomes prevalent, inmates will be able to hold visual call with their families for up to 30 minutes a week. For the time being, online visu...\n",
      "\n",
      "      ğŸ“„ Document 3:\n",
      "         ğŸ¯ Rank: 232539306671, Shard: 0\n",
      "         ğŸ“ Length: 269 words\n",
      "         ğŸ“ƒ Text: \\section{Introduction}\n",
      "\n",
      "Eliciting predictions of uncertain events from \\emph{experts}\n",
      "or other knowledgeable agents---or \n",
      "relevant information pertaining to events---is a fundamental \n",
      "problem of study in statistics, economics, operations research,\n",
      " artificial intelligence and a variety of other area...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ” N-gram 2: 'intelligence and machine'\n",
      "   ğŸ“ˆ Total occurrences: 360\n",
      "   ğŸ“„ Documents retrieved: 3\n",
      "   ğŸ“ Top documents:\n",
      "\n",
      "      ğŸ“„ Document 1:\n",
      "         ğŸ¯ Rank: 19605443884, Shard: 0\n",
      "         ğŸ“ Length: 671 words\n",
      "         ğŸ“ƒ Text: Memorial Healthcare System\n",
      "Ganesh Persad is the Manager of Clinical Systems & Interoperability at Memorial Healthcare System (MHS). He serves as the technical architect who manages various health information exchange platforms such as Epicâ€™s Care Everywhere, Carequality, eHealth Exchange, Florida He...\n",
      "\n",
      "      ğŸ“„ Document 2:\n",
      "         ğŸ¯ Rank: 19605443885, Shard: 0\n",
      "         ğŸ“ Length: 703 words\n",
      "         ğŸ“ƒ Text: said.\n",
      "â€œWe are going to build the secure, intelligent platform off which you can run your digital business of the future,â€ Robbins said.\n",
      "The new kind of intuitive network is powered by intent and informed by context. What does that mean? Imagine you are an IT or network administrator and want to depl...\n",
      "\n",
      "      ğŸ“„ Document 3:\n",
      "         ğŸ¯ Rank: 19605443886, Shard: 0\n",
      "         ğŸ“ Length: 703 words\n",
      "         ğŸ“ƒ Text: said.\n",
      "â€œWe are going to build the secure, intelligent platform off which you can run your digital business of the future,â€ Robbins said.\n",
      "The new kind of intuitive network is powered by intent and informed by context. What does that mean? Imagine you are an IT or network administrator and want to depl...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ” N-gram 3: 'and machine learning'\n",
      "   ğŸ“ˆ Total occurrences: 1,615\n",
      "   ğŸ“„ Documents retrieved: 3\n",
      "   ğŸ“ Top documents:\n",
      "\n",
      "      ğŸ“„ Document 1:\n",
      "         ğŸ¯ Rank: 179595127225, Shard: 0\n",
      "         ğŸ“ Length: 695 words\n",
      "         ğŸ“ƒ Text: quire Human Element due to Domain Complexity and Poor Process Discipline\n",
      "By 2025, 30% of new legal technology automation solutions will combine software with staffing for a â€œhuman-in-the-loopâ€ offering, according to Gartner, Inc. Despite growing demand for greater automation and increasing sophistic...\n",
      "\n",
      "      ğŸ“„ Document 2:\n",
      "         ğŸ¯ Rank: 179595127226, Shard: 0\n",
      "         ğŸ“ Length: 621 words\n",
      "         ğŸ“ƒ Text: ï¿½ï¿½,ç„¡è«–æ˜¯ç·šä¸Šç”¢æ¥­ä¸å¯æˆ–ç¼ºçš„å¤§æ•¸æ“šæ•´åˆã€é æ¸¬è¡Œæ¥­è¶¨å‹¢æä¾›æœ‰æ•ˆè³‡è¨Š;æˆ–æ˜¯è³‡é‡‘\n",
      "èˆ‡æ•¸æ“šçš„å®‰å…¨ä¿è­·å•é¡Œ,äº¦èƒ½åˆ©ç”¨AIä¾†é˜²æ­¢ç¶²è·¯æ”»æ“Šã€‚ä¸”çœ‹ä»¥ä¸‹äº”å¤§å°‡å½±éŸ¿åšå¥•ç”¢æ¥­çš„AIæŠ€è¡“! !\n",
      "As the development of artificial intelligence grows increasingly more sophisticated and begins to become\n",
      "embedded in a number of solutions, the potential for its use in the gambling sector is becoming more\n",
      "apparent...\n",
      "\n",
      "      ğŸ“„ Document 3:\n",
      "         ğŸ¯ Rank: 179595127227, Shard: 0\n",
      "         ğŸ“ Length: 467 words\n",
      "         ğŸ“ƒ Text: Help doctors to make better medical decisions, how does machine learning work?\n",
      "Home > Technology > Machine learning > Help doctors to make better medical decisions, how does machine learning work?\n",
      "In most cases, big data is regarded by one of the major hospitals as one of the least important abiliti...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ” N-gram 4: 'machine learning are'\n",
      "   ğŸ“ˆ Total occurrences: 136\n",
      "   ğŸ“„ Documents retrieved: 3\n",
      "   ğŸ“ Top documents:\n",
      "\n",
      "      ğŸ“„ Document 1:\n",
      "         ğŸ¯ Rank: 313625094008, Shard: 0\n",
      "         ğŸ“ Length: 767 words\n",
      "         ğŸ“ƒ Text: technologies such as artificial intelligence (AI) and machine learning (ML).\n",
      "AI and ML are used to make intelligent decisions about key business and banking processes. They may also be used to analyze large amounts of data in order to determine the creditworthiness of an application. Additionally, A...\n",
      "\n",
      "      ğŸ“„ Document 2:\n",
      "         ğŸ¯ Rank: 313625094009, Shard: 0\n",
      "         ğŸ“ Length: 651 words\n",
      "         ğŸ“ƒ Text: emost goal of mobile service providers. In Korea, we have already achieved that.â€ With 5G the network â€œwill be a lot of different types of traffic that need to be able to connect. In order to support those different types of traffic â€¦ it will require a lot of work. Thatâ€™s why we are looking at data ...\n",
      "\n",
      "      ğŸ“„ Document 3:\n",
      "         ğŸ¯ Rank: 313625094010, Shard: 0\n",
      "         ğŸ“ Length: 376 words\n",
      "         ğŸ“ƒ Text: up the rate of advanced prostate cancers. Another problem with skipping screening is that it can deny men with low-grade prostate cancers the option of undergoing active surveillance so they will know if their disease starts growing quickly and needs more aggressive therapy,\" he said.\n",
      "In the years a...\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ” N-gram 5: 'learning are transforming'\n",
      "   ğŸ“ˆ Total occurrences: 13\n",
      "   ğŸ“„ Documents retrieved: 2\n",
      "   ğŸ“ Top documents:\n",
      "\n",
      "      ğŸ“„ Document 1:\n",
      "         ğŸ¯ Rank: 175652238324, Shard: 0\n",
      "         ğŸ“ Length: 664 words\n",
      "         ğŸ“ƒ Text: ing keynote: Redefining power\n",
      "Nilanjana Roy, Novelist and FT Life & Arts columnist\n",
      "Money management: the great missed opportunity\n",
      "Women are vastly outnumbered by men in asset management. They occupy an estimated one in ten roles, and about 90 per cent of the money in funds is managed by men. Why doe...\n",
      "\n",
      "      ğŸ“„ Document 2:\n",
      "         ğŸ¯ Rank: 175652238325, Shard: 0\n",
      "         ğŸ“ Length: 664 words\n",
      "         ğŸ“ƒ Text: ing keynote: Redefining power\n",
      "Nilanjana Roy, Novelist and FT Life & Arts columnist\n",
      "Money management: the great missed opportunity\n",
      "Women are vastly outnumbered by men in asset management. They occupy an estimated one in ten roles, and about 90 per cent of the money in funds is managed by men. Why doe...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example: Search for documents containing n-grams from this text\n",
    "sample_text = \"\"\"\n",
    "Artificial intelligence and machine learning are transforming how we process natural language.\n",
    "Neural networks can now understand context and generate human-like responses.\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸš€ Starting document retrieval...\")\n",
    "print(f\"ğŸ“ Input text: {sample_text.strip()}\")\n",
    "\n",
    "# Get top 3 documents for each n-gram\n",
    "results = get_top_documents_for_ngrams(sample_text, max_ngram_size=3, top_k=3)\n",
    "\n",
    "# Display results\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7deaed7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a clean, focused implementation for:\n",
    "\n",
    "1. **N-gram Extraction**: Breaking input text into 2-4 gram phrases\n",
    "2. **Document Search**: Using Infini-gram API to find documents containing each n-gram  \n",
    "3. **Document Retrieval**: Getting the top 3 documents with actual text content for each n-gram\n",
    "4. **Results Display**: Showing results in a clear, organized format\n",
    "\n",
    "### Key Features:\n",
    "- âœ… Extracts meaningful n-grams from any input text\n",
    "- âœ… Queries the Infini-gram API with proper error handling\n",
    "- âœ… Retrieves actual document text content (not just counts)\n",
    "- âœ… Returns top 3 documents per n-gram with metadata\n",
    "- âœ… Displays results in an easy-to-read format\n",
    "\n",
    "### Usage:\n",
    "```python\n",
    "# Basic usage\n",
    "results = get_top_documents_for_ngrams(\"Your text here\", max_ngram_size=3, top_k=3)\n",
    "display_results(results)\n",
    "```\n",
    "\n",
    "The notebook is now ready for production use for n-gram-based document discovery and retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
