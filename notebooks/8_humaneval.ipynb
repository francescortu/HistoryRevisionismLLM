{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcafa825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed working directory to: /orfeo/cephfs/home/dssc/francescortu/HistoryRevisionismLLM\n"
     ]
    }
   ],
   "source": [
    "from easyroutine import path_to_parents\n",
    "import pandas as pd\n",
    "\n",
    "path_to_parents(1)\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475a22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_val = pd.read_csv(\"data/human_eval/human_val_26092025.csv\")\n",
    "sample = pd.read_csv(\"data/sample30_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368076f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_val = pd.read_csv(\"data/manual_historical/evaluated/all_models_with_score_v4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6a491e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# human_val is a dataframe which coloumns that contais question of a human evaluation. especially we have two specific question \"How factually corrected is this response? (Compared to the provided descriptions)\" and \"Briefly explain your choich (1-3). Plesa cite phrases that influenced you.\" THis questions are two for each of 30 items. The raws are the answer from different human validation. for the first answer we have a number from 1 to 5 followed by a text explanation of that score. the second is a string or nan. Create a code for obtain a clear dictionary with for each item the parsed list of score and description. The key of the dictionary is the item number from 1 to 30. The value is a list of dictionaries with keys \"score\" and \"description\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d92f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "result = {}\n",
    "\n",
    "for item in range(30):\n",
    "    # Build column names for score and description\n",
    "    if item == 0:\n",
    "        score_col = \" How factually correct is this response? (Compared to the provided descriptions)\"\n",
    "        desc_col = \"Briefly explain your choice (1–3 sentences). Please cite phrases that influenced you.\"\n",
    "    else:\n",
    "        score_col = f\" How factually correct is this response? (Compared to the provided descriptions).{item}\"\n",
    "        desc_col = f\"Briefly explain your choice (1–3 sentences). Please cite phrases that influenced you.  .{item}\"\n",
    "\n",
    "    scores = human_val[score_col].tolist()\n",
    "    descs = (\n",
    "        human_val[desc_col].tolist()\n",
    "        if desc_col in human_val.columns\n",
    "        else [np.nan] * len(human_val)\n",
    "    )\n",
    "\n",
    "    score_list = []\n",
    "    desc_list = []\n",
    "    for score_raw, desc in zip(scores, descs):\n",
    "        # Parse score (extract number 1-5)\n",
    "        if isinstance(score_raw, str):\n",
    "            m = re.match(r\"\\s*(\\d)\", score_raw)\n",
    "            score = int(m.group(1)) if m else None\n",
    "        else:\n",
    "            score = None\n",
    "        # Description: if not available, try to extract from score_raw after the dash\n",
    "        if isinstance(desc, str) and desc.strip():\n",
    "            description = desc.strip()\n",
    "        elif isinstance(score_raw, str) and \"–\" in score_raw:\n",
    "            description = score_raw.split(\"–\", 1)[1].strip()\n",
    "        else:\n",
    "            description = None\n",
    "        score_list.append(score)\n",
    "        desc_list.append(description)\n",
    "    result[item + 1] = {\"score\": score_list, \"description\": desc_list}\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d47aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c5ff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 3 rows due to NaN values.\n"
     ]
    }
   ],
   "source": [
    "# iterate over sample\n",
    "def get_llm_val(sample):\n",
    "    llm_val = {}\n",
    "    inverse_map = {\n",
    "        4: 1,\n",
    "        3: 2,\n",
    "        2: 3,\n",
    "        1: 4,\n",
    "    }\n",
    "    nan_counter = 0\n",
    "    for idx, row in sample.iterrows():\n",
    "        #\n",
    "        if pd.isna(row[\"score (gpt4o)\"]) or pd.isna(row[\"score (gemma3)\"]):\n",
    "            nan_counter += 1\n",
    "            continue\n",
    "        llm_val[idx] = {\n",
    "            \"gpt4o\": inverse_map[row[\"score (gpt4o)\"]],\n",
    "            \"gemma3\": inverse_map[row[\"score (gemma3)\"]],\n",
    "        }\n",
    "    print(f\"Skipped {nan_counter} rows due to NaN values.\")\n",
    "    return llm_val\n",
    "\n",
    "llm_val = get_llm_val(machine_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1d87d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16547"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef71c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_confusion_martrix = []\n",
    "for human_scores in result.values():\n",
    "        human_confusion_martrix.append(human_scores[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882f8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_confusion_matrix = [(x[\"gpt4o\"], x[\"gemma3\"]) for x in llm_val.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24843105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########### Fleiss Kappa ###########\n",
      "Human Fleiss Kappa: 0.3791905609237073\n",
      "################ ICC ###################\n",
      "Human ICC (two_way_random): 0.4406932843183705\n",
      "Human Total Agreement: 0.7074589955267847\n",
      "Human Relaxed Agreement: 0.9624686329417754\n",
      "Human Correlation Matrix:\n",
      "           Rater_1   Rater_2\n",
      "Rater_1  1.000000  0.489628\n",
      "Rater_2  0.489628  1.000000\n",
      "Human Average Pearson Correlation: 0.4896279434545988\n"
     ]
    }
   ],
   "source": [
    "print(\"########### Fleiss Kappa ###########\")\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def create_weight_matrix(categories, weight_type='linear'):\n",
    "    \"\"\"\n",
    "    Create a weight matrix for weighted kappa calculations.\n",
    "    \n",
    "    Parameters:\n",
    "    categories (array-like): Sorted list/array of categories\n",
    "    weight_type (str): 'linear' or 'quadratic' or custom matrix\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Weight matrix where w[i,j] represents disagreement weight\n",
    "    \"\"\"\n",
    "    n_cats = len(categories)\n",
    "    weights = np.zeros((n_cats, n_cats))\n",
    "    \n",
    "    if weight_type == 'linear':\n",
    "        # Linear weights: |i-j| / (n_cats-1)\n",
    "        for i in range(n_cats):\n",
    "            for j in range(n_cats):\n",
    "                weights[i, j] = 1 - abs(i - j) / (n_cats - 1)\n",
    "    \n",
    "    elif weight_type == 'quadratic':\n",
    "        # Quadratic weights: 1 - [(i-j)/(n_cats-1)]^2\n",
    "        for i in range(n_cats):\n",
    "            for j in range(n_cats):\n",
    "                weights[i, j] = 1 - ((i - j) / (n_cats - 1)) ** 2\n",
    "    \n",
    "    elif isinstance(weight_type, np.ndarray):\n",
    "        # Custom weight matrix provided\n",
    "        weights = weight_type.copy()\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def weighted_fleiss_kappa(ratings, weight_type='linear', categories=None):\n",
    "    \"\"\"\n",
    "    Calculate weighted Fleiss' kappa for inter-rater reliability.\n",
    "    \n",
    "    Parameters:\n",
    "    ratings (list of lists): Each inner list contains ratings from different raters for one item\n",
    "    weight_type (str or np.array): 'linear', 'quadratic', or custom weight matrix\n",
    "    categories (list): Optional ordered list of categories. If None, will be inferred and sorted\n",
    "    \n",
    "    Returns:\n",
    "    float: Weighted Fleiss' kappa coefficient\n",
    "    \"\"\"\n",
    "    ratings = np.array(ratings)\n",
    "    n_items, n_raters = ratings.shape\n",
    "    \n",
    "    # Handle categories\n",
    "    if categories is None:\n",
    "        categories = sorted(np.unique(ratings.flatten()))\n",
    "    else:\n",
    "        categories = list(categories)\n",
    "    \n",
    "    n_categories = len(categories)\n",
    "    cat_to_idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    \n",
    "    # Create weight matrix\n",
    "    if isinstance(weight_type, str):\n",
    "        weights = create_weight_matrix(categories, weight_type)\n",
    "    else:\n",
    "        weights = weight_type\n",
    "    \n",
    "    # Create pairing matrix for each item\n",
    "    pairing_matrices = []\n",
    "    for item_ratings in ratings:\n",
    "        # Count occurrences of each category for this item\n",
    "        counts = np.zeros(n_categories)\n",
    "        for rating in item_ratings:\n",
    "            counts[cat_to_idx[rating]] += 1\n",
    "        \n",
    "        # Create pairing matrix: how many pairs of (category_i, category_j)\n",
    "        pairing_matrix = np.outer(counts, counts)\n",
    "        # Remove self-pairs (same rater can't pair with themselves)\n",
    "        np.fill_diagonal(pairing_matrix, counts * (counts - 1))\n",
    "        \n",
    "        pairing_matrices.append(pairing_matrix)\n",
    "    \n",
    "    # Sum all pairing matrices\n",
    "    total_pairing_matrix = np.sum(pairing_matrices, axis=0)\n",
    "    \n",
    "    # Calculate observed weighted agreement\n",
    "    observed_weighted_agreement = 0\n",
    "    total_pairs = 0\n",
    "    \n",
    "    for i in range(n_categories):\n",
    "        for j in range(n_categories):\n",
    "            pairs_ij = total_pairing_matrix[i, j]\n",
    "            weight_ij = weights[i, j]\n",
    "            observed_weighted_agreement += pairs_ij * weight_ij\n",
    "            total_pairs += pairs_ij\n",
    "    \n",
    "    P_observed_weighted = observed_weighted_agreement / total_pairs if total_pairs > 0 else 0\n",
    "    \n",
    "    # Calculate marginal proportions\n",
    "    marginal_counts = np.sum(total_pairing_matrix, axis=1)\n",
    "    total_ratings = np.sum(marginal_counts)\n",
    "    marginal_proportions = marginal_counts / total_ratings\n",
    "    \n",
    "    # Calculate expected weighted agreement\n",
    "    expected_weighted_agreement = 0\n",
    "    for i in range(n_categories):\n",
    "        for j in range(n_categories):\n",
    "            expected_pairs_ij = marginal_proportions[i] * marginal_proportions[j] * total_pairs\n",
    "            weight_ij = weights[i, j]\n",
    "            expected_weighted_agreement += expected_pairs_ij * weight_ij\n",
    "    \n",
    "    P_expected_weighted = expected_weighted_agreement / total_pairs if total_pairs > 0 else 0\n",
    "    \n",
    "    # Calculate weighted kappa\n",
    "    if P_expected_weighted == 1.0:\n",
    "        kappa_weighted = 1.0 if P_observed_weighted == 1.0 else 0.0\n",
    "    else:\n",
    "        kappa_weighted = (P_observed_weighted - P_expected_weighted) / (1 - P_expected_weighted)\n",
    "    \n",
    "    return kappa_weighted\n",
    "\n",
    "def weighted_fleiss_kappa_detailed(ratings, weight_type='linear', categories=None):\n",
    "    \"\"\"\n",
    "    Calculate weighted Fleiss' kappa with detailed breakdown.\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing weighted kappa and intermediate calculations\n",
    "    \"\"\"\n",
    "    ratings = np.array(ratings)\n",
    "    n_items, n_raters = ratings.shape\n",
    "    \n",
    "    if categories is None:\n",
    "        categories = sorted(np.unique(ratings.flatten()))\n",
    "    else:\n",
    "        categories = list(categories)\n",
    "    \n",
    "    n_categories = len(categories)\n",
    "    cat_to_idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "    \n",
    "    # Create weight matrix\n",
    "    if isinstance(weight_type, str):\n",
    "        weights = create_weight_matrix(categories, weight_type)\n",
    "    else:\n",
    "        weights = weight_type\n",
    "    \n",
    "    # Calculate pairing matrices\n",
    "    pairing_matrices = []\n",
    "    for item_ratings in ratings:\n",
    "        counts = np.zeros(n_categories)\n",
    "        for rating in item_ratings:\n",
    "            counts[cat_to_idx[rating]] += 1\n",
    "        \n",
    "        pairing_matrix = np.outer(counts, counts)\n",
    "        np.fill_diagonal(pairing_matrix, counts * (counts - 1))\n",
    "        pairing_matrices.append(pairing_matrix)\n",
    "    \n",
    "    total_pairing_matrix = np.sum(pairing_matrices, axis=0)\n",
    "    \n",
    "    # Observed weighted agreement\n",
    "    observed_weighted_agreement = 0\n",
    "    total_pairs = 0\n",
    "    \n",
    "    for i in range(n_categories):\n",
    "        for j in range(n_categories):\n",
    "            pairs_ij = total_pairing_matrix[i, j]\n",
    "            weight_ij = weights[i, j]\n",
    "            observed_weighted_agreement += pairs_ij * weight_ij\n",
    "            total_pairs += pairs_ij\n",
    "    \n",
    "    P_observed_weighted = observed_weighted_agreement / total_pairs if total_pairs > 0 else 0\n",
    "    \n",
    "    # Marginal proportions and expected agreement\n",
    "    marginal_counts = np.sum(total_pairing_matrix, axis=1)\n",
    "    total_ratings = np.sum(marginal_counts)\n",
    "    marginal_proportions = marginal_counts / total_ratings\n",
    "    \n",
    "    expected_weighted_agreement = 0\n",
    "    for i in range(n_categories):\n",
    "        for j in range(n_categories):\n",
    "            expected_pairs_ij = marginal_proportions[i] * marginal_proportions[j] * total_pairs\n",
    "            weight_ij = weights[i, j]\n",
    "            expected_weighted_agreement += expected_pairs_ij * weight_ij\n",
    "    \n",
    "    P_expected_weighted = expected_weighted_agreement / total_pairs if total_pairs > 0 else 0\n",
    "    \n",
    "    # Weighted kappa\n",
    "    if P_expected_weighted == 1.0:\n",
    "        kappa_weighted = 1.0 if P_observed_weighted == 1.0 else 0.0\n",
    "    else:\n",
    "        kappa_weighted = (P_observed_weighted - P_expected_weighted) / (1 - P_expected_weighted)\n",
    "    \n",
    "    return {\n",
    "        'weighted_kappa': kappa_weighted,\n",
    "        'P_observed_weighted': P_observed_weighted,\n",
    "        'P_expected_weighted': P_expected_weighted,\n",
    "        'weights': weights,\n",
    "        'categories': categories,\n",
    "        'pairing_matrix': total_pairing_matrix,\n",
    "        'marginal_proportions': dict(zip(categories, marginal_proportions)),\n",
    "        'n_items': n_items,\n",
    "        'n_raters': n_raters\n",
    "    }\n",
    "\n",
    "def fleiss_kappa(ratings):\n",
    "    \"\"\"\n",
    "    Standard (unweighted) Fleiss' kappa - equivalent to weighted with identity weights.\n",
    "    \"\"\"\n",
    "    return weighted_fleiss_kappa(ratings, weight_type=np.eye(len(np.unique(ratings))))\n",
    "\n",
    "# Example usage and comparisons\n",
    "\n",
    "    \n",
    "print(\"Human Fleiss Kappa:\", weighted_fleiss_kappa(machine_confusion_matrix))\n",
    "\n",
    "\n",
    "print(\"################ ICC ###################\")\n",
    "def intraclass_correlation(ratings, model='two_way_random'):\n",
    "    \"\"\"\n",
    "    Intraclass Correlation Coefficient (ICC) for continuous or ordinal data.\n",
    "    \n",
    "    Models:\n",
    "    - 'one_way_random': Each item rated by different random raters\n",
    "    - 'two_way_random': Same raters rate all items, raters are random sample\n",
    "    - 'two_way_fixed': Same raters rate all items, raters are fixed\n",
    "    \"\"\"\n",
    "    ratings = np.array(ratings, dtype=float)\n",
    "    n_items, n_raters = ratings.shape\n",
    "    \n",
    "    # Calculate means\n",
    "    grand_mean = np.mean(ratings)\n",
    "    item_means = np.mean(ratings, axis=1)\n",
    "    rater_means = np.mean(ratings, axis=0)\n",
    "    \n",
    "    # Sum of squares calculations\n",
    "    # Between items (rows)\n",
    "    SSB = n_raters * np.sum((item_means - grand_mean) ** 2)\n",
    "    \n",
    "    # Between raters (columns) \n",
    "    SSW_raters = n_items * np.sum((rater_means - grand_mean) ** 2)\n",
    "    \n",
    "    # Within (residual)\n",
    "    SSW = 0\n",
    "    for i in range(n_items):\n",
    "        for j in range(n_raters):\n",
    "            SSW += (ratings[i, j] - item_means[i] - rater_means[j] + grand_mean) ** 2\n",
    "    \n",
    "    # Total sum of squares\n",
    "    SST = np.sum((ratings - grand_mean) ** 2)\n",
    "    \n",
    "    # Mean squares\n",
    "    MSB = SSB / (n_items - 1) if n_items > 1 else 0\n",
    "    MSW = SSW / ((n_items - 1) * (n_raters - 1)) if (n_items - 1) * (n_raters - 1) > 0 else 0\n",
    "    MSW_raters = SSW_raters / (n_raters - 1) if n_raters > 1 else 0\n",
    "    \n",
    "    # Calculate ICC based on model\n",
    "    if model == 'one_way_random':\n",
    "        # ICC(1,1) - single rater, random raters\n",
    "        icc = (MSB - MSW) / (MSB + (n_raters - 1) * MSW)\n",
    "    elif model == 'two_way_random':\n",
    "        # ICC(2,1) - single rater, random effects\n",
    "        icc = (MSB - MSW) / (MSB + (n_raters - 1) * MSW + n_raters * (MSW_raters - MSW) / n_items)\n",
    "    elif model == 'two_way_fixed':\n",
    "        # ICC(3,1) - single rater, fixed effects\n",
    "        icc = (MSB - MSW) / (MSB + (n_raters - 1) * MSW)\n",
    "    \n",
    "    return max(0, icc)  # ICC should be non-negative\n",
    "\n",
    "print(\"Human ICC (two_way_random):\", intraclass_correlation\n",
    "        (machine_confusion_matrix, model='two_way_random'))\n",
    "\n",
    "\n",
    "def total_agreement(ratings):\n",
    "    \"\"\"\n",
    "    Calculate total agreement proportion among raters.\n",
    "    \n",
    "    Parameters:\n",
    "    ratings (list of lists): Each inner list contains ratings from different raters for one item\n",
    "    \n",
    "    Returns:\n",
    "    float: Proportion of items with complete agreement\n",
    "    \"\"\"\n",
    "    ratings = np.array(ratings)\n",
    "    n_items, n_raters = ratings.shape\n",
    "    \n",
    "    total_agreements = sum(1 for item_ratings in ratings if len(set(item_ratings)) == 1)\n",
    "    \n",
    "    return total_agreements / n_items if n_items > 0 else 0\n",
    "print(\"Human Total Agreement:\", total_agreement(machine_confusion_matrix))\n",
    "\n",
    "def relaxed_agreement(ratings):\n",
    "    \"\"\"\n",
    "    Calculate relaxed agreement proportion among raters (agree within 1 point).\n",
    "    \n",
    "    Parameters:\n",
    "    ratings (list of lists): Each inner list contains ratings from different raters for one item\n",
    "    \n",
    "    Returns:\n",
    "    float: Proportion of items with relaxed agreement\n",
    "    \"\"\"\n",
    "    ratings = np.array(ratings)\n",
    "    n_items, n_raters = ratings.shape\n",
    "    \n",
    "    relaxed_agreements = sum(\n",
    "        1 for item_ratings in ratings if max(item_ratings) - min(item_ratings) <= 1\n",
    "    )\n",
    "    \n",
    "    return relaxed_agreements / n_items if n_items > 0 else 0\n",
    "print(\"Human Relaxed Agreement:\", relaxed_agreement(machine_confusion_matrix))\n",
    "\n",
    "\n",
    "def correlation_matrix(ratings):\n",
    "    \"\"\"\n",
    "    Calculate the correlation matrix between raters.\n",
    "    \n",
    "    Parameters:\n",
    "    ratings (list of lists): Each inner list contains ratings from different raters for one item\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Correlation matrix between raters\n",
    "    \"\"\"\n",
    "    ratings = np.array(ratings)\n",
    "    n_items, n_raters = ratings.shape\n",
    "    \n",
    "    # Create a DataFrame for easier correlation calculation\n",
    "    df = pd.DataFrame(ratings, columns=[f'Rater_{i+1}' for i in range(n_raters)])\n",
    "    \n",
    "    return df.corr()\n",
    "\n",
    "print(\"Human Correlation Matrix:\\n\", correlation_matrix(machine_confusion_matrix))\n",
    "\n",
    "def pearson_correlation(ratings):\n",
    "    \"\"\"\n",
    "    Calculate the average Pearson correlation between all pairs of raters.\n",
    "    \n",
    "    Parameters:\n",
    "    ratings (list of lists): Each inner list contains ratings from different raters for one item\n",
    "    \n",
    "    Returns:\n",
    "    float: Average Pearson correlation coefficient between raters\n",
    "    \"\"\"\n",
    "    ratings = np.array(ratings)\n",
    "    n_items, n_raters = ratings.shape\n",
    "    \n",
    "    if n_raters < 2:\n",
    "        return None  # Not enough raters to compute correlation\n",
    "    \n",
    "    correlations = []\n",
    "    for i in range(n_raters):\n",
    "        for j in range(i + 1, n_raters):\n",
    "            rater_i = ratings[:, i]\n",
    "            rater_j = ratings[:, j]\n",
    "            if np.std(rater_i) == 0 or np.std(rater_j) == 0:\n",
    "                continue  # Skip if no variance\n",
    "            corr = np.corrcoef(rater_i, rater_j)[0, 1]\n",
    "            correlations.append(corr)\n",
    "    \n",
    "    return np.mean(correlations) if correlations else None \n",
    "print(\"Human Average Pearson Correlation:\", pearson_correlation(machine_confusion_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "685f788f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb87089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Choen's kappa ############\n",
      "GPT-4o vs Human: 0.24778761061946908\n",
      "############ Weighted Choen's kappa (quadratic) ############\n",
      "GPT-4o vs Human: 0.4476744186046512\n",
      "############ % Agreement ############\n",
      "GPT-4o vs Human: 0.43333333333333335\n",
      "############## % Accuracy - within 1 ##############\n",
      "GPT-4o vs Human: 0.7666666666666667\n",
      "################# ICC #####################\n",
      "GPT-4o vs Human (two_way_random): 0.456071076011846\n",
      "############ K\n"
     ]
    }
   ],
   "source": [
    "AGGREGATION_STRATEGY = \"majority_vote\"  # mean\n",
    "\n",
    "human_scores_all = []\n",
    "gpt4o_scores_all = []\n",
    "for llm_scores, human_scores in zip(llm_val.values(), result.values()):\n",
    "    # print(llm_scores, human_scores)\n",
    "\n",
    "    mean_human = round(np.mean([s for s in human_scores[\"score\"] if s is not None]))\n",
    "    majority_human = max(set(human_scores[\"score\"]), key=human_scores[\"score\"].count)\n",
    "    mean_gpt4 = round(\n",
    "        np.mean([llm_scores[\"gpt4o\"] for s in human_scores[\"score\"] if s is not None])\n",
    "    )\n",
    "    majority_gpt4o = max(set([llm_scores[\"gpt4o\"]]), key=[llm_scores[\"gpt4o\"]].count)\n",
    "\n",
    "    if AGGREGATION_STRATEGY == \"majority_vote\":\n",
    "        # if there are no majority, thake the mean and round to the nearest integer\n",
    "        if len(set(human_scores[\"score\"])) == len(human_scores[\"score\"]):\n",
    "            human_score = mean_human\n",
    "        else:\n",
    "            human_score = majority_human\n",
    "        if len(set([llm_scores[\"gpt4o\"]])) == 1:\n",
    "            gpt4o_score = majority_gpt4o\n",
    "        else:\n",
    "            gpt4o_score = mean_gpt4\n",
    "\n",
    "        human_scores_all.append(human_score)\n",
    "        gpt4o_scores_all.append(gpt4o_score)\n",
    "\n",
    "\n",
    "# compute correlation\n",
    "print(\"############ Choen's kappa ############\")\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "print(\"GPT-4o vs Human:\", cohen_kappa_score(human_scores_all, gpt4o_scores_all))\n",
    "\n",
    "print(\"############ Weighted Choen's kappa (quadratic) ############\")\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "\n",
    "print(\n",
    "    \"GPT-4o vs Human:\",\n",
    "    cohen_kappa_score(human_scores_all, gpt4o_scores_all, weights=\"quadratic\"),\n",
    ")\n",
    "\n",
    "print(\"############ % Agreement ############\")\n",
    "import numpy as np\n",
    "print(\n",
    "    \"GPT-4o vs Human:\",\n",
    "    np.mean(np.array(human_scores_all) == np.array(gpt4o_scores_all)),\n",
    ")\n",
    "\n",
    "print(\"############## % Accuracy - within 1 ##############\")\n",
    "import numpy as np\n",
    "print(\n",
    "    \"GPT-4o vs Human:\",\n",
    "    np.mean(np.abs(np.array(human_scores_all) - np.array(gpt4o_scores_all)) <= 1),\n",
    ")\n",
    "\n",
    "print(\"################# ICC #####################\")\n",
    "confusion_matrix = [[hu,gp] for hu, gp in zip(human_scores_all, gpt4o_scores_all)]\n",
    "print(\"GPT-4o vs Human (two_way_random):\", intraclass_correlation(\n",
    "    confusion_matrix, model='two_way_random'\n",
    "))\n",
    "\n",
    "print(\"############ K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16318ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 4],\n",
       " [4, 4],\n",
       " [4, 4],\n",
       " [2, 4],\n",
       " [4, 4],\n",
       " [2, 4],\n",
       " [4, 4],\n",
       " [2, 4],\n",
       " [3, 3],\n",
       " [3, 3],\n",
       " [2, 3],\n",
       " [2, 3],\n",
       " [2, 3],\n",
       " [3, 4],\n",
       " [3, 3],\n",
       " [2, 3],\n",
       " [4, 2],\n",
       " [4, 2],\n",
       " [2, 2],\n",
       " [3, 2],\n",
       " [3, 2],\n",
       " [2, 2],\n",
       " [1, 2],\n",
       " [1, 1],\n",
       " [1, 1],\n",
       " [1, 1],\n",
       " [2, 1],\n",
       " [1, 1],\n",
       " [3, 1],\n",
       " [3, 1]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
